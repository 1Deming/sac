# Soft Actor-Critic
Soft actor-critic is a deep reinforcement learning framework for training maximum entropy policies in continous domains. [TODO: paper reference]

# Installation
TODO

# Examples
TODO

# Credits
The soft actor-critic algorithm was developed by Tuomas Haarnoja under the supervision of [Prof. Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/) and [Prof. Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/) at UC Berkeley. Special thanks to [Vitchyr Pong](https://github.com/vitchyr), who wrote some parts of the code, and [Kristian Hartikainen](https://github.com/hartikainen) who helped testing and documenting the code. The work was supported by [Berkeley Deep Drive](https://deepdrive.berkeley.edu/).

# Reference
TODO